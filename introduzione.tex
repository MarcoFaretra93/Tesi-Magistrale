Nell'ultimo decennio, grandi, medie e piccole aziende si trovano sempre più di fronte alla sfida di dover produrre, trattare ed utilizzare una quantità sempre maggiore di dati, ciò è dovuto principalmente alla diffusione di strumenti tecnologici, all'aumento della capacità di calcolo ed allo sviluppo di internet, dei sensori e delle reti di comunicazione. \newline
Tale crescita porta ad una proliferazione delle sorgenti dei dati disponibili, con un conseguente incremento delle mole di dati da gestire e potenzialmente utilizzare per fini operativi e decisionali. Questo fenomeno, spesso conosciuto come Big Data, ha introdotto diversi problemi: i tempi di risposta sono aumentati drasticamente; lo spazio e l'hardware cominciano a non bastare più. \newline
L'importanza della conoscenza è stata evidente fin dagli anni '70 e il suo utilizzo si è presto diffuso in molti ambiti industriali e tecnici, tra cui il supporto alle decisioni e il data warehousing. \newline
Al giorno d'oggi, l'avvento dei Big Data, ha reso ancora più essenziale elaborare e trattare tale conoscenza, con il fine ultimo di produrne una nuova. \newline
In effetti, il termine attuale "economia della conoscenza" indica proprio l'insieme delle attività umane volte a trarre valore, tangibile e intangibile, da tale conoscenza. La conoscenza, e quindi l'informazione che la costituisce, è sempre più vista come forza motrice nelle attività di business. Ci sono molte aziende informatiche che hanno come business plan la conoscenza, ad esempio aziende che vendono dati estratti dal web, grosse aziende e banche che vendono e utilizzano i dati per sofisticate inferenze relative ai propri clienti, con vari fini: profilazione, mantenimento della clientela, sviluppo di sistemi di recommendation, contrasto alle frodi, vendita dei dati a terze parti e molto altro. \newline
L'esplosione è avvenuta negli ultimi anni, in passato erano presenti diverse difficoltà, sia tecniche che teoriche, i linguaggi di programmazione erano molto complessi e gli ingegneri erano in numero limitato, l'hardware, i database, erano inadeguati, rappresentavano un collo di bottiglia per le elaborazioni troppo grandi. \newline
Con il passare degli anni, l'avvento tecnologico ha subito una crescita esponenziale, l'hardware si è evoluto, le tecnologie dei database sono migliorate notevolmente, i linguaggi di programmazione sono diventati molto più semplici, sono nati nuovi paradigmi di programmazione, inoltre è possibile utilizzare framework architetturali che fungono da middleware e permettono quindi la riduzione di scrittura di codice da parte di uno sviluppatore. Ovviamente questa semplificazione ha portato ad un enorme vantaggio nell'elaborazione di conoscenza e reasoning su grandi quantità di dati. \newline
Il termine Knowledge Graph è stato originariamente riferito solo a quello di Google, ovvero "una base di conoscenze utilizzata da Google per migliorare i risultati di ricerca del suo motore, con informazioni di ricerca semantica raccolte da una grande varietà di fonti". Nel frattempo, altri colossi come Facebook, Amazon, ecc., hanno costruito i proprio Knowledge Graph, e molte altre aziende vorrebbero mantenere un knowledge graph privato aziendale che contiene molte quantità di dati in forma di fatti. Tale Knowledge Graph aziendale dovrebbe contenere conoscenze di business rilevanti, ad esempio su clienti, prodotti, prezzi, ecc. Questo dovrebbe essere gestito da un KGMS, cioè un sistema di gestione delle basi di conoscenza e che svolge altri task basati su grandi quantità di dati fornendo strumenti per il data analytics e il machine learning. La parola '\textit{graph}' in questo contesto viene spesso fraintesa: molti credono che avere un Graph Database System e alimentare tale database con i dati, sia sufficiente per ottenere un Knowledge Graph aziendale, altri pensano che siano limitati alla memorizzazione e all'analisi dei dati, ma non dovrebbero essere limitati a questo~\cite{bellomarini2017swift}. \newline
Come definito in~\cite{bellomarini2017swift} definiamo i requisiti per avere un KGMS completo. Esso deve svolgere compiti complessi di reasoning, ed allo stesso tempo ottenere performance efficienti e scalabili sui Big Data con una complessit\`a computazionale accettabile. Inoltre necessita di interfacce con i database aziendali, il web e il machine learning. Il core di un KGMS deve fornire un linguaggio per rappresentare la conoscenza e il resoning.\newline \newline
Introdurremo un sistema che soddisfa tutti i requisiti del KGMS sopra citati, ed utilizza un linguaggio che fa parte della famiglia del Datalog. \newline
Datalog~\cite{atzeni2006basi} è un linguaggio di interrogazione per basi di dati che ha riscosso un notevole interesse dalla metà degli anni ottanta, è basato su regole di deduzione. Rappresenta un linguaggio sottoinsieme del linguaggio Prolog relativo ai database relazionali, anche Datalog è basato su regole di deduzione ma non permette l'utilizzo né di simboli di funzione né di un modello di valutazione. \newline
Ogni regola è composta da una testa (chiamata anche head o conseguente) e da un corpo (chiamato anche body o antecedente) a loro volta formati da uno o più predicati atomici. Se tutti gli atomi del corpo sono verificati, ne consegue che anche il predicato atomico della testa lo sia. L'espressività di Datalog è dovuta alla possibilità di scrivere regole ricorsive, cioè in grado di richiamare il medesimo atomo della testa anche nel corpo della regola. \newline
Uno dei migliori linguaggi per il reasoning basato sulla conoscenza è Datalog. Durante gli anni è stato studiato in dettaglio, nel data exchange e data integration~\cite{furche2016data}. \newline
Il problema di Datalog è che non permette una forte potenza espressiva, è quindi stata progettata una famiglia di linguaggi, chiamata Datalog$^\pm$ che aggiunge maggiore potenza espressiva al linguaggio nativo, mantenendo costante efficienza e scalabilità~\cite{bellomarini2017swift}. \newline 
La famiglia Datalog$^\pm$ estende Datalog con quantificatori esistenziali nelle teste delle regole, ed allo stesso tempo limita la sua sintassi in modo da ottenere decidibilità e scalabilità dei dati~\cite{cali2013taming,cali2012towards,cali2010datalog+}.  \newline \newline
Il sistema che presentiamo ed utilizziamo in questa tesi, Vadalog Reasoner, è il contributo dell'università di Oxford al progetto VADA~\cite{VADA}, in collaborazione con le università di Edimburgo e Manchester. La mia attività è stata svolta presso il Laboratorio basi di dati dell'università degli studi Roma Tre, lavorando da remoto con il mio correlatore Luigi Bellomarini. \newline
Il Vadalog Reasoner è un KGMS, che offre un motore centrale di reasoning principale ed un linguaggio, il Vadalog, per la gestione e l'utilizzo. \newline
Vadalog appartiene alla famiglia Datalog$^\pm$ sopra descritta ed è in grado di soddisfare tutti i requisiti di un KGMS completo sopra definiti. \newline
Il core logico del Vadalog Reasoner è in grado di processare tale linguaggio, è in grado di eseguire task di reasoning ontologici e risulta computazionalmente efficiente, tale da soddisfare i requisiti citati, esso ha inoltre accesso ad un repository di regole. \newline
Il problema principale nell'utilizzo del Datalog$^\pm$ è che l'introduzione della quantificazione esistenziale nelle teste porta subito all'indecidibilità. Sono quindi nati una serie di frammenti (cioè delle restrizioni) di Datalog$^\pm$ che sono decidibili. \newline
Tali frammenti hanno diversa complessità computazionale (data complexity), esponenziale nel caso più generale. \newline
Uno dei punti di forza di Datalog è la bassa complessità come descritto in precedenza, Vadalog cattura ed estende Datalog senza aumentare tale complessità, questo avviene grazie alle proprietà ereditate dal frammento Warded Datalog$^\pm$, il linguaggio su cui si basa Vadalog. \newline
In termini intuitivi, il Warded Datalog$^\pm$ (così come altri frammenti di Datalog$^\pm$) contiene la propagazione dei valori nulli nell'ambito della ricorsione. Il linguaggio riesce ad imporre tali vincoli grazie a semplici restrizioni sintattiche, che individuano quali variabili possono contenere valori nulli che si propagano nella testa (variabili \emph{dangerous}), e impongono che tali variabili debbano sempre comparire esattamente in un atomo del corpo (detto \emph{ward}) che interagisce con gli altri atomi del corpo solo mediante variabili che non possono assumere valori nulli (\emph{harmless})~\cite{bellomarini2017swift}. \newline
Il Vadalog Reasoner fornisce anche degli strumenti che permettono il data analytics, l'iniezione di codice procedurale, l'integrazione con diverse tipologie di input (ad esempio database relazionali, file csv, database non relazionali, ecc.).\newline \newline
Il mio contributo in questa tesi è stato l'implementazione di feature core del Vadalog Reasoner per l'esecuzione di programmi Vadalog. \newline
Inizialmente il progetto presentava un ambito di intervento molto vasto. \newline
Non erano presenti diversi tipi di dati, anche primitivi, per permettere all'utente di effettuare maggiori statistiche. Tale requisito è fondamentale se si vogliono integrare tipi di dati nel linguaggio Vadalog. \newline
Non erano presenti delle ottimizzazioni per permettere un guadagno sull'esecuzione di programmi Vadalog. Ciò rappresentava un limite volendo guadagnare in efficienza e permettere quindi delle performance migliori. Ad esempio non veniva ottimizzata efficientemente la ricorsione. \newline
Erano stati effettuati soltanto dei benchmark iniziali per testare le perfomance, ma nulla di concreto per effettuare anche confronti con i competitor esistenti, e quindi capire se il sistema fosse competitivo nel settore. \newline
Le sorgenti a disposizione dell'utente finale erano in numero limitato. Anche questo problema rappresentava un collo di bottiglia per il nostro sistema, in quanto l'utente aveva poche sorgenti per incrociare i dati ed effettuarne analisi. \newline
Non era possibile integrare codice sorgente da altri linguaggi, né definire funzioni all'interno del linguaggio Vadalog. Ciò era un limite, poiché non permetteva di fare computazioni laboriose o di utilizzare del codice già scritto in precedenza (magari dedicato ad un calcolo ben definito). \newline
Era presente un'interfaccia web molto scarna e con poche funzionalità. L'utente aveva accesso soltanto a poche funzionalità rispetto a tutte le funzionalità fornite dal sistema. \medskip  \newline
Di seguito una breve lista delle funzionalità di cui mi sono occupato, che verranno illustrate in maniera più approfondita nei prossimi capitoli:
\begin{itemize}
	\item Implementazione di nuovi tipi di dato, semplici e strutturati, per risolvere il problema dei pochi tipi di dato disponibili nel sistema.
	\item Tecniche di ottimizzazione in Datalog per migliorare le prestazioni. In particolare mi sono occupato delle ottimizzazioni di: \emph{Push Selections e Projections Down}, ovvero la trasformazione di una query in un'altra equivalente, ma anticipando le selezioni e le proiezioni alle regole più vicine a quelle di input (ad esempio l'ideale sarebbe inserire le selezioni e le proiezioni alle regole di input), questo risulta efficiente poiché coinvolge un numero minore di tuple nell'operazione di join~\cite{atzeni2006basi}; supporto per ricorsione sinistra e destra, trasformazione dove le ricorsioni destre (nel corpo di una regola l'atomo che ricorre si trova alla destra di tutti gli altri atomi) che risultano più problematiche anche a livello di computazione temporale, vengono opportunamente trasformate in ricorsioni sinistre (nel corpo di una regola l'atomo che ricorre si trova alla sinistra di tutti gli altri atomi) che risultano più efficienti; ottimizzazione multi-join, ovvero quando si è in presenza di regole che hanno al loro interno join condivisi tra tre o più atomi, essi vengono splittati in un numero di regole proporzionale al numero di atomi, in modo di avere al più regole con un join tra due atomi.
	\item Creazione di benchmark per effettuare test sulle performance per effettuare analisi esaustive sulle performance del sistema ed effettuare infine confronti con altri sistemi esistenti. Mi sono occupato inoltre dello sviluppo di \textit{iWarded}, un piccolo tool che genera programmi Warded Datalog$^\pm $ per effettuare benchmark. Tale sistema prende in input diversi parametri che hanno lo scopo di descrivere le particolarità del programma Vadalog, e ne restituisce quest'ultimo.
	\item Supporto di nuove sorgenti, file CSV e database relazionali e non relazionali. Questo permette maggiore interazione per l'utente, è così possibile combinare i dati provenienti da storage eterogenei ed effettuare statistiche e tutte le operazioni offerte dal sistema su di essi.
	\item Supporto alle user-defined functions. È possibile definire delle funzioni all'interno di Vadalog, al quale è possibile passare dei parametri, ed è possibile esprimere la correlazione tra esse e funzioni esterne che possono essere implementate in diversi linguaggi. Essenziali per l'utente se vuole integrare funzioni all'interno del programma Vadalog, ad esempio funzioni già scritte in passato che calcolano una determinata statistica.
	\item Miglioramento notevole dell'interfaccia web (Vadalog console), con l'integrazione di elementi grafici che permettono l'interazione con tutti i servizi offerti dal sistema, integrazione di un editor e tante altre funzionalità. \newline
\end{itemize} 
La tesi è organizzata come segue: \newline \newline
Il capitolo 1 descrive il Vadalog Engine, in particolare le proprietà di un KGMS ed in modo approfondito l'architettura del nostro sistema. \newline \newline
Il capitolo 2 è incentrato sul linguaggio Vadalog, in particolare è presente un approfondimento più ampio sul core logico della famiglia di linguaggi Datalog$^\pm$ e di Vadalog, della complessità e delle estensioni di quest'ultimo. \newline \newline
Il capitolo 3 è dedicato alla descrizione nel dettaglio delle funzionalità riguardanti il mio contributo al progetto. \newline \newline
Il capitolo 4 descrive le prove sperimentali effettuate sul sistema Vadalog. \newline \newline
Il capitolo 5 tratta del paragone effettuato tra il sistema Vadalog ed i competitor già presenti sul mercato. \newline \newline
Il capitolo 6 trae le conclusioni del lavoro.
